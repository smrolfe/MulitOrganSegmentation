{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4079953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from glob import glob\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import Dataset, DataLoader, create_test_image_3d, decollate_batch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import UNETR\n",
    "\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    SaveImaged,\n",
    "    Spacingd,\n",
    "    CropForegroundd,\n",
    "    EnsureTyped,\n",
    "    ScaleIntensityRanged,\n",
    "    ToTensord\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca505e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = \"/home/sara/MONAI/KOMP/data/imagesTs\"\n",
    "images = sorted(glob(os.path.join(tempdir, \"A*.nii.gz\")))\n",
    "files = [{\"image\": image} for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbcd827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pre transforms\n",
    "pre_transforms = Compose([\n",
    "    LoadImaged(keys=\"image\"),\n",
    "    Spacingd(keys=[\"image\"], pixdim=(\n",
    "            1, 1, 1), mode=\"bilinear\"),\n",
    "    EnsureChannelFirstd(keys=\"image\"),\n",
    "    Orientationd(keys=\"image\", axcodes=\"RAS\"),\n",
    "    ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=-175, a_max=250, \n",
    "            b_min=0.0, b_max=1.0, clip=True),\n",
    "    CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "    ToTensord(keys=[\"image\"]),\n",
    "    ])\n",
    "    \n",
    "# define post transforms\n",
    "post_transforms = Compose([\n",
    "    EnsureTyped(keys=\"pred\"),\n",
    "    Activationsd(keys=\"pred\", sigmoid=True),\n",
    "    Invertd(\n",
    "        keys=\"pred\",\n",
    "        transform=pre_transforms,\n",
    "        orig_keys=\"image\",\n",
    "        meta_keys=\"pred_meta_dict\", \n",
    "        orig_meta_keys=\"image_meta_dict\",\n",
    "        meta_key_postfix=\"meta_dict\",  \n",
    "        nearest_interp=False, \n",
    "        to_tensor=True,  \n",
    "    ),\n",
    "    AsDiscreted(keys=\"pred\", threshold=0.5),\n",
    "    SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=\"./out\", output_postfix=\"seg\", resample=False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05db2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset and dataloader\n",
    "dataset = Dataset(data=files, transform=pre_transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "torch.set_num_threads(24)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = \"/home/sara/MONAI/KOMP\"\n",
    "\n",
    "net = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=51,\n",
    "    img_size=(96, 96, 96),\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_heads=12,\n",
    "    pos_embed=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "net.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for d in dataloader:\n",
    "        images = d[\"image\"].to(device)\n",
    "        # define sliding window size and batch size for windows inference\n",
    "        d[\"pred\"] = sliding_window_inference(inputs=images, roi_size=(96, 96, 96), sw_batch_size=4, predictor=net, \n",
    "            overlap=0.8)\n",
    "        # decollate the batch data into a list of dictionaries, then execute postprocessing transforms\n",
    "        d = [post_transforms(i) for i in decollate_batch(d)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c18031",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    torch.argmax(d, dim=1).detach().cpu()[0, :, :, slice_map[img_name]]\n",
    "    )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
